{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c553486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyyadav/miniforge3/envs/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from typing import Annotated,TypedDict\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.tools import tool\n",
    "import os\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c607b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Optional\n",
    "\n",
    "def now_iso():\n",
    "    \"\"\"Return current datetime in ISO format including time and timezone if available.\"\"\"\n",
    "    return datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
    "\n",
    "class AnalysisSource(BaseModel):\n",
    "    source_name:  Optional[str] = Field(..., description=\"Name of the data source, e.g., Google, Bing, Reddit, Google Finance\")\n",
    "    analysis: str = Field(..., description=\"Analysis result or summary from this source\")\n",
    "    source_link: Optional[str] = Field(..., description=\"Direct link to the information or post\")\n",
    "\n",
    "\n",
    "class LLMAnalysisResult(BaseModel):\n",
    "    user_question: str = Field(..., description=\"The user's original question\")\n",
    "    sources: list[AnalysisSource] = Field(..., description=\"List of sources with analysis and URLs\")\n",
    "    synthesized_answer: str = Field(..., description=\"LLM's synthesized answer using all sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf1ee0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGoogleGenerativeAI(model='gemini-2.5-flash',api_key=os.getenv('GEMINI_API_KEY'),async_client=True)\n",
    "analysing_llm=llm.with_structured_output(AnalysisSource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "973c7f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ethics in AI is a critical and rapidly evolving field focused on ensuring that artificial intelligence systems are developed, deployed, and used in a manner that aligns with human values, societal well-being, and moral principles. It grapples with profound questions like algorithmic bias, which can perpetuate and amplify existing societal inequalities; the imperative for transparency and explainability in AI decision-making; and the complex issues of accountability when AI systems make errors or cause harm. Addressing these ethical dilemmas is paramount to fostering public trust, preventing unintended negative consequences, protecting individual privacy and autonomy, and ultimately ensuring that AI serves as a beneficial tool that enhances, rather than diminishes, human dignity and fairness across all domains.', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-98302b1d-b01c-40b0-8538-b60959a16e23-0', usage_metadata={'input_tokens': 9, 'output_tokens': 136, 'total_tokens': 1043})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await llm.ainvoke('give me aa para about ethics in ai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e572ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"The rapid advancement of Artificial Intelligence inextricably links its development with profound ethical considerations. Ensuring fairness and mitigating algorithmic bias, often stemming from unrepresentative training data, is paramount to prevent discrimination and perpetuate societal inequalities. Equally crucial are establishing clear lines of accountability for AI's actions, promoting transparency in its decision-making processes, and safeguarding user privacy in data collection and utilization. Furthermore, the ethical deployment of AI demands careful consideration of its societal impact, from potential job displacement to the need for maintaining human oversight over autonomous systems, preventing misuse that could undermine democratic processes or exacerbate social inequalities. Ultimately, integrating ethical principles into AI's design, deployment, and governance is not merely a philosophical exercise, but a critical imperative to ensure this transformative technology serves humanity beneficially and equitably.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9899bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "await analysing_llm.ainvoke(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e972f",
   "metadata": {},
   "source": [
    "## Define state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f0df997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    messages:Annotated[str,add_messages]\n",
    "    user_question:str|None\n",
    "    google_search_results:str|None\n",
    "    google_finance_results:str|None\n",
    "    bing_search_results:str|None\n",
    "    reddit_search_results:str|None\n",
    "    selected_reddit_urls:list[str]|None\n",
    "    reddit_posts:str|None\n",
    "    google_analysis:str\n",
    "    bing_analysis:str\n",
    "    reddit_analysis:str\n",
    "    google_finance_analysis:str\n",
    "    synthesized_answer:str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74aec2c",
   "metadata": {},
   "source": [
    "# NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3afd6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Return\n",
    "\n",
    "\n",
    "def google_search(state:ResearchState)->ResearchState:\n",
    "    return state\n",
    "\n",
    "\n",
    "def bing_search(state:ResearchState)->ResearchState:\n",
    "    return state\n",
    "\n",
    "\n",
    "def reddit_search(state:ResearchState)->ResearchState:\n",
    "    return state\n",
    "\n",
    "def twitter_search(state: ResearchState) -> ResearchState:\n",
    "    return state\n",
    "\n",
    "def google_finance_search(state:ResearchState)->ResearchState:\n",
    "    return state\n",
    "\n",
    "def analysis_google_results(state:ResearchState)->ResearchState:\n",
    "    return state\n",
    "\n",
    "\n",
    "def analysis_bing_results(state:ResearchState)->ResearchState:\n",
    "    return state\n",
    "\n",
    "\n",
    "def analysis_reddit_results(state:ResearchState)->ResearchState:\n",
    "    return state\n",
    "\n",
    "\n",
    "def analyze_results_results(state:ResearchState)->ResearchState:\n",
    "    return state\n",
    "    \n",
    "\n",
    "def synthesize_results(state:ResearchState)->ResearchState:\n",
    "    return state\n",
    "\n",
    "\n",
    "def final_results(state:ResearchState)->ResearchState:\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c1894",
   "metadata": {},
   "source": [
    "# GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ba2e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph=StateGraph(ResearchState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fedd359",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_node(\"google_search\", google_search)\n",
    "graph.add_node(\"bing_search\", bing_search)\n",
    "graph.add_node(\"reddit_search\", reddit_search)\n",
    "graph.add_node(\"analysis_google_results\", analysis_google_results)\n",
    "graph.add_node(\"analysis_bing_results\", analysis_bing_results)\n",
    "graph.add_node(\"analysis_reddit_results\", analysis_reddit_results)\n",
    "graph.add_node(\"analyze_results_results\", analyze_results_results)\n",
    "graph.add_node(\"synthesize_results\", synthesize_results)\n",
    "graph.add_node(\"final_results\", final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_edge(START, \"google_search\")\n",
    "graph.add_edge(START, \"bing_search\")\n",
    "graph.add_edge(START, \"reddit_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b8f5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncpraw\n",
    "import aiocache\n",
    "from aiocache import Cache,caches\n",
    "\n",
    "client_id = '2ns033kTrWIIpDJSBDJZYw'\n",
    "client_secret = '0Jm2zTDhaBV6VNbGhyg0PPTnlnD-SQ'\n",
    "user_agent = 'dp by /u/Temporary_Version105 '\n",
    "\n",
    "# --- Configure aiocache ---\n",
    "# We'll use a simple file-based cache stored in a folder named './cache_storage'\n",
    "# It will use JSON to store the data (serializer)\n",
    "caches.set_config({\n",
    "    'default': {\n",
    "        'cache': \"aiocache.cache.FileCache\",\n",
    "        'cache_directory': \"./cache_storage\",\n",
    "        'serializer': {\n",
    "            'class': \"aiocache.serializers.JsonSerializer\"\n",
    "        }\n",
    "    }\n",
    "})\n",
    "async def praw():\n",
    "    \"\"\"The main async function, now using aiocache.\"\"\"\n",
    "    cache=caches.get('default')\n",
    "    \n",
    "    print(\"Connecting to Reddit...\")\n",
    "    reddit = asyncpraw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "\n",
    "    subreddit = await reddit.subreddit('finance')\n",
    "    \n",
    "    post_processed=0\n",
    "\n",
    "    async for post in subreddit.hot(limit=50):\n",
    "        post_processed+=1\n",
    "\n",
    "        cached_data=await caches.get(post.id)\n",
    "\n",
    "        if cached_data:\n",
    "        \n",
    "            print(f\"  > Using cached data for post: {cached_data['title']}\")\n",
    "\n",
    "        else:\n",
    "            comments=[]\n",
    "            await post.load()\n",
    "\n",
    "            for comment in post.comments.list()[:10]:\n",
    "                if hasattr(comment,'body'):\n",
    "                       comments.append(comment.body)\n",
    "\n",
    "            print(post_processed)\n",
    "            post_data = {\"title\": post.title, \"score\": post.score, \"comments\": comments}\n",
    "\n",
    "            await cache.set(post.id, post_data, ttl=3600)\n",
    "        \n",
    "    await cache.close()\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cb7b946",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aiocache.cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m praw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m praw()\n",
      "Cell \u001b[0;32mIn[45], line 23\u001b[0m, in \u001b[0;36mpraw\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpraw\u001b[39m():\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The main async function, now using aiocache.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     cache\u001b[38;5;241m=\u001b[39m\u001b[43mcaches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnecting to Reddit...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     reddit \u001b[38;5;241m=\u001b[39m asyncpraw\u001b[38;5;241m.\u001b[39mReddit(\n\u001b[1;32m     27\u001b[0m         client_id\u001b[38;5;241m=\u001b[39mclient_id,\n\u001b[1;32m     28\u001b[0m         client_secret\u001b[38;5;241m=\u001b[39mclient_secret,\n\u001b[1;32m     29\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent\n\u001b[1;32m     30\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/new_env/lib/python3.10/site-packages/aiocache/factory.py:171\u001b[0m, in \u001b[0;36mCacheHandler.get\u001b[0;34m(self, alias)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    170\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_alias_config(alias)\n\u001b[0;32m--> 171\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[43m_create_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_caches[alias] \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache\n",
      "File \u001b[0;32m~/miniforge3/envs/new_env/lib/python3.10/site-packages/aiocache/factory.py:33\u001b[0m, in \u001b[0;36m_create_cache\u001b[0;34m(cache, serializer, plugins, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m _class_from_string(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m     31\u001b[0m         plugins_instances\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplugin))\n\u001b[0;32m---> 33\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[43m_class_from_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cache, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m cache\n\u001b[1;32m     34\u001b[0m instance \u001b[38;5;241m=\u001b[39m cache(serializer\u001b[38;5;241m=\u001b[39mserializer, plugins\u001b[38;5;241m=\u001b[39mplugins_instances, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "File \u001b[0;32m~/miniforge3/envs/new_env/lib/python3.10/site-packages/aiocache/factory.py:17\u001b[0m, in \u001b[0;36m_class_from_string\u001b[0;34m(class_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m class_name \u001b[38;5;241m=\u001b[39m class_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m module_name \u001b[38;5;241m=\u001b[39m class_path\u001b[38;5;241m.\u001b[39mrstrip(class_name)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfromlist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, class_name)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aiocache.cache'"
     ]
    }
   ],
   "source": [
    "praw=await praw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd28e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiocache[file] in /Users/divyyadav/miniforge3/envs/new_env/lib/python3.10/site-packages (0.12.3)\n",
      "\u001b[33mWARNING: aiocache 0.12.3 does not provide the extra 'file'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"aiocache[file]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a004a731",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1725026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eb ih'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[::-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9785bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1oh6tsf\n",
      "1oevvop\n",
      "1obb89t\n",
      "1o87gos\n",
      "1o7dul7\n",
      "1o5k55a\n",
      "1o5as1r\n",
      "1o2swix\n",
      "1o2n7yn\n",
      "1o2h0ih\n"
     ]
    }
   ],
   "source": [
    "for key,val in cache.items():\n",
    "    print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
